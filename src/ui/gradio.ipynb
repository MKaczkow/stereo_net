{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from StereoNet import StereoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.7.2 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file d:\\__repos\\stereo_net\\src\\ui\\best-model.ckpt`\n"
     ]
    }
   ],
   "source": [
    "# Load your PyTorch model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # model = torch.load(\"./best-model.ckpt\")\n",
    "    model = StereoNet.load_from_checkpoint(\"./best-model.ckpt\")\n",
    "    # model = torch.load(\"./src/ui/best-model.ckpt\")\n",
    "else:\n",
    "    model = StereoNet.load_from_checkpoint(\"./best-model.ckpt\", map_location=torch.device('cpu'))\n",
    "    # model = torch.load(\"./best-model.ckpt\", map_location=torch.device('cpu'))\n",
    "    # model = torch.load(\"./src/ui/best-model.ckpt\", map_location=torch.device('cpu'))\n",
    "# model = torch.load(\"../model/BEST-epoch=23-step=864006.ckpt\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale():\n",
    "    \"\"\"\n",
    "    Rescales the left and right image tensors (initially ranged between [0, 1]) and rescales them to be between [-1, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(sample):\n",
    "        for name in ['left', 'right']:\n",
    "            sample[name] = (sample[name] - 0.5) * 2\n",
    "        return sample\n",
    "    \n",
    "def png_loader(path):\n",
    "    img = Image.open(path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert PNG to WebP\n",
    "def prepare_sample(left_image, right_imate):\n",
    "\n",
    "    # left_image = png_loader(left_image)\n",
    "    # right_imate = png_loader(right_imate)\n",
    "    # transforms = [Rescale(), ToTensor()]\n",
    "    left_image = ToTensor()(left_image)\n",
    "    right_imate = ToTensor()(right_imate)\n",
    "    left_image = left_image.unsqueeze(0)\n",
    "    right_imate = right_imate.unsqueeze(0)\n",
    "    transforms = [Rescale()]\n",
    "\n",
    "    sample = {\n",
    "        'left': left_image, 'right': right_imate, \n",
    "        }\n",
    "\n",
    "    # torch_sample = ToTensor()(sample)\n",
    "    for transform in transforms:\n",
    "        sample = transform(sample)\n",
    "        # left_image = transform(left_image)\n",
    "        # right_imate = transform(right_imate)\n",
    "\n",
    "\n",
    "    return sample\n",
    "\n",
    "def disparity_estimation(left_image, right_image):\n",
    "\n",
    "    sample = prepare_sample(left_image, right_image)\n",
    "    # disparity_map = model(sample['left'], sample['right'])\n",
    "    with torch.no_grad():\n",
    "        disparity_map = model(sample)\n",
    "    \n",
    "    print(type(disparity_map))\n",
    "    print(disparity_map.shape)\n",
    "    print(disparity_map.max)\n",
    "    print(disparity_map.min)\n",
    "\n",
    "    disparity_map = disparity_map.squeeze().numpy()\n",
    "\n",
    "    return disparity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maciek\\AppData\\Local\\Temp\\ipykernel_15240\\874370193.py:5: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Image(type=\"pil\", label=\"Left Image (PNG)\"),\n",
      "C:\\Users\\Maciek\\AppData\\Local\\Temp\\ipykernel_15240\\874370193.py:5: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Image(type=\"pil\", label=\"Left Image (PNG)\"),\n",
      "C:\\Users\\Maciek\\AppData\\Local\\Temp\\ipykernel_15240\\874370193.py:6: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  gr.inputs.Image(type=\"pil\", label=\"Right Image (PNG)\"),\n",
      "C:\\Users\\Maciek\\AppData\\Local\\Temp\\ipykernel_15240\\874370193.py:6: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  gr.inputs.Image(type=\"pil\", label=\"Right Image (PNG)\"),\n",
      "C:\\Users\\Maciek\\AppData\\Local\\Temp\\ipykernel_15240\\874370193.py:8: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  outputs=gr.outputs.Image(type=\"pil\", label=\"Disparity Map (PNG)\"),\n"
     ]
    }
   ],
   "source": [
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=disparity_estimation,\n",
    "    inputs=[\n",
    "        gr.inputs.Image(type=\"pil\", label=\"Left Image (PNG)\"),\n",
    "        gr.inputs.Image(type=\"pil\", label=\"Right Image (PNG)\"),\n",
    "    ],\n",
    "    outputs=gr.outputs.Image(type=\"pil\", label=\"Disparity Map (PNG)\"),\n",
    "    title=\"Disparity Estimation\",\n",
    "    description=\"Upload two PNG images for left and right views to estimate disparity.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 540, 960])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\routes.py\", line 488, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1434, in process_api\n",
      "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1341, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\components\\image.py\", line 314, in postprocess\n",
      "    return processing_utils.encode_array_to_base64(y)\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\processing_utils.py\", line 104, in encode_array_to_base64\n",
      "    pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))\n",
      "  File \"d:\\__repos\\stereo_net\\venv\\lib\\site-packages\\gradio\\processing_utils.py\", line 416, in _convert\n",
      "    raise ValueError(\"Images of type float must be between -1 and 1.\")\n",
      "ValueError: Images of type float must be between -1 and 1.\n"
     ]
    }
   ],
   "source": [
    "# Start the Gradio UI\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
