{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your PyTorch model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.load(\"./best-model.ckpt\")\n",
    "    # model = torch.load(\"./src/ui/best-model.ckpt\")\n",
    "else:\n",
    "    model = torch.load(\"./best-model.ckpt\", map_location=torch.device('cpu'))\n",
    "    # model = torch.load(\"./src/ui/best-model.ckpt\", map_location=torch.device('cpu'))\n",
    "# model = torch.load(\"../model/BEST-epoch=23-step=864006.ckpt\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale():\n",
    "    \"\"\"\n",
    "    Rescales the left and right image tensors (initially ranged between [0, 1]) and rescales them to be between [-1, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __call__(sample):\n",
    "        for name in ['left', 'right']:\n",
    "            sample[name] = (sample[name] - 0.5) * 2\n",
    "        return sample\n",
    "    \n",
    "def png_loader(path):\n",
    "    img = io.imread(path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert PNG to WebP\n",
    "def prepare_sample(left_image, right_imate):\n",
    "\n",
    "    left_image = png_loader(left_image)\n",
    "    right_imate = png_loader(right_imate)\n",
    "\n",
    "    sample = {\n",
    "        'left': left_image, 'right': right_imate, \n",
    "        }\n",
    "    transforms = [Rescale()]\n",
    "\n",
    "    torch_sample = ToTensor()(sample)\n",
    "    for transform in transforms:\n",
    "        torch_sample = transform(torch_sample)\n",
    "\n",
    "    return torch_sample\n",
    "\n",
    "def disparity_estimation(left_image, right_image):\n",
    "\n",
    "    sample = prepare_sample(left_image, right_image)\n",
    "    # disparity_map = model(sample['left'], sample['right'])\n",
    "    with torch.no_grad():\n",
    "        disparity_map = model(sample)\n",
    "    return disparity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=disparity_estimation,\n",
    "    inputs=[\n",
    "        gr.inputs.Image(type=\"pil\", label=\"Left Image (PNG)\"),\n",
    "        gr.inputs.Image(type=\"pil\", label=\"Right Image (PNG)\"),\n",
    "    ],\n",
    "    outputs=gr.outputs.Image(type=\"pil\", label=\"Disparity Map (PNG)\"),\n",
    "    title=\"Disparity Estimation\",\n",
    "    description=\"Upload two PNG images for left and right views to estimate disparity.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Gradio UI\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
